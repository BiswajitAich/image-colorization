{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Biswajit Aich\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__) # 2.18.0\n",
    "print(cv2.__version__) # 4.11.0\n",
    "print(np.__version__) # 2.0.2\n",
    "print(Image.__version__) # 11.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"calibretaliation/colorization\", split=f\"train[5000:10000]\")\n",
    "ds = ds.remove_columns([col for col in ds.column_names if col not in [\"original_image\", \"colorized_image\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_preprocess(image, target_size=(32, 32)):\n",
    "    image = np.array(image)\n",
    "    if image.ndim == 2:\n",
    "        image = np.stack([image] * 3, axis=-1)\n",
    "\n",
    "    image = tf.convert_to_tensor(image, dtype=tf.float32)\n",
    "    image = tf.image.resize(image, target_size)\n",
    "    image = (image / 127.5) - 1\n",
    "\n",
    "    return image\n",
    "\n",
    "def process_dataset(example):\n",
    "    return {\n",
    "        'colorized_image': resize_and_preprocess(example['colorized_image'], (64, 64)),\n",
    "        'original_image': resize_and_preprocess(example['original_image'], (64, 64))\n",
    "        }\n",
    "\n",
    "ds = ds.map(process_dataset, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_colored = np.array([item for item in ds['colorized_image']])\n",
    "train_bw = np.array([item for item in ds['original_image']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_image(generated_images, original_images, num_images=3):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Display original grayscale images\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(2, num_images, i + 1)\n",
    "        gray_img = np.squeeze(original_images[i])\n",
    "        plt.imshow(((gray_img + 1) * 127.5).astype(np.uint8), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title('Original')\n",
    "\n",
    "    # Display generated color images\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(2, num_images, num_images + i + 1)\n",
    "        color_img = generated_images[i]\n",
    "        plt.imshow(((color_img + 1) * 127.5).astype(np.uint8))\n",
    "        plt.axis('off')\n",
    "        plt.title('Generated')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "view_image(train_colored[67:70], train_bw[67:70], num_images=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DISCRIMINATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(shape=(64, 64, 3)):\n",
    "    image = tf.keras.layers.Input(shape=shape, name='input_image')\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), strides=(1, 1), padding='same')(image)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(256, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(256, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=image, outputs=x, name='Discriminator')\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "d_model = discriminator(shape=(64, 64, 3))\n",
    "d_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(d_model, show_shapes=True, show_layer_names=True, show_layer_activations=True, dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(dataset, n_samples, is_real):\n",
    "    ix = np.random.randint(0, dataset.shape[0], n_samples)\n",
    "    X = dataset[ix]\n",
    "    y = np.ones((n_samples, 1)) if is_real else np.zeros((n_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(input_shape=(64, 64, 3)):\n",
    "    input_image = tf.keras.layers.Input(shape=input_shape, name='input_image')\n",
    "\n",
    "    down1 = tf.keras.layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same')(input_image)\n",
    "    bn1 = tf.keras.layers.BatchNormalization()(down1)\n",
    "    act1 = tf.keras.layers.LeakyReLU(alpha=0.2)(bn1)\n",
    "\n",
    "    down2 = tf.keras.layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same')(act1)\n",
    "    bn2 = tf.keras.layers.BatchNormalization()(down2)\n",
    "    act2 = tf.keras.layers.LeakyReLU(alpha=0.2)(bn2)\n",
    "\n",
    "    down3 = tf.keras.layers.Conv2D(256, (3, 3), strides=(2, 2), padding='same')(act2)\n",
    "    bn3 = tf.keras.layers.BatchNormalization()(down3)\n",
    "    act3 = tf.keras.layers.LeakyReLU(alpha=0.2)(bn3)\n",
    "\n",
    "    down4 = tf.keras.layers.Conv2D(256, (3, 3), strides=(2, 2), padding='same')(act3)\n",
    "    bn4 = tf.keras.layers.BatchNormalization()(down4)\n",
    "    act4 = tf.keras.layers.LeakyReLU(alpha=0.2)(bn4)\n",
    "\n",
    "    up = tf.keras.layers.Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same')(act4)\n",
    "    bn_up = tf.keras.layers.BatchNormalization()(up)\n",
    "    drop = tf.keras.layers.Dropout(0.2)(bn_up)\n",
    "    relu_up = tf.keras.layers.ReLU()(drop)\n",
    "    skip = tf.keras.layers.Concatenate()([relu_up, act3])\n",
    "\n",
    "    up1 = tf.keras.layers.Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same')(skip)\n",
    "    bn_up1 = tf.keras.layers.BatchNormalization()(up1)\n",
    "    drop1 = tf.keras.layers.Dropout(0.2)(bn_up1)\n",
    "    relu_up1 = tf.keras.layers.ReLU()(drop1)\n",
    "    skip1 = tf.keras.layers.Concatenate()([relu_up1, act2])\n",
    "\n",
    "    up2 = tf.keras.layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same')(skip1)\n",
    "    bn_up2 = tf.keras.layers.BatchNormalization()(up2)\n",
    "    drop2 = tf.keras.layers.Dropout(0.2)(bn_up2)\n",
    "    relu_up2 = tf.keras.layers.ReLU()(drop2)\n",
    "    skip2 = tf.keras.layers.Concatenate()([relu_up2, act1])\n",
    "\n",
    "    up3 = tf.keras.layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same')(skip2)\n",
    "    bn_up3 = tf.keras.layers.BatchNormalization()(up3)\n",
    "    drop3 = tf.keras.layers.Dropout(0.2)(bn_up3)\n",
    "    relu_up3 = tf.keras.layers.ReLU()(drop3)\n",
    "\n",
    "    output = tf.keras.layers.Conv2D(3, (3, 3), activation='tanh', padding='same')(relu_up3)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_image, outputs=output, name='Generator')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(g_model, show_shapes=True, show_layer_names=True, show_layer_activations=True, dpi=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_generator_samples(g_model, dataset, n_samples, is_real=False):\n",
    "    ix = np.random.randint(0, dataset.shape[0], n_samples)\n",
    "    X = dataset[ix]\n",
    "    gen_images = g_model.predict(X, verbose=0)\n",
    "    y = np.ones((n_samples, 1)) if is_real else np.zeros((n_samples, 1))\n",
    "    return gen_images, y\n",
    "\n",
    "vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet', input_shape=(64, 64, 3))\n",
    "vgg.trainable = False\n",
    "vgg_feature_extractor = tf.keras.Model(inputs=vgg.input, outputs=vgg.get_layer('block3_conv3').output)\n",
    "\n",
    "@tf.function(reduce_retracing=True)\n",
    "def perceptual_loss(y_true, y_pred):\n",
    "    y_true_processed = tf.keras.applications.vgg19.preprocess_input(y_true * 255.0)\n",
    "    y_pred_processed = tf.keras.applications.vgg19.preprocess_input(y_pred * 255.0)\n",
    "\n",
    "    features_true = vgg_feature_extractor(y_true_processed)\n",
    "    features_pred = vgg_feature_extractor(y_pred_processed)\n",
    "\n",
    "    return tf.reduce_mean(tf.abs(features_true - features_pred))\n",
    "\n",
    "\n",
    "def l1_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "def gan_loss(disc_generated_output):\n",
    "    return bce(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "def combined_generator_loss(y_true, y_pred, disc_generated_output, lambda_l1=100.0, lambda_perc=10.0):\n",
    "    l1 = l1_loss(y_true, y_pred)\n",
    "    perc = perceptual_loss(y_true, y_pred)\n",
    "    g_loss = gan_loss(disc_generated_output)\n",
    "\n",
    "    total_loss = g_loss + (lambda_l1 * l1) + (lambda_perc * perc)\n",
    "    return total_loss\n",
    "\n",
    "def define_gan(g_model, d_model, shape):\n",
    "    d_model.trainable = False\n",
    "\n",
    "    gan_input = tf.keras.layers.Input(shape=shape)\n",
    "    g_output = g_model(gan_input)\n",
    "\n",
    "    d_output = d_model(g_output)\n",
    "\n",
    "    model = tf.keras.Model(inputs=gan_input, outputs=[g_output, d_output], name='GAN')\n",
    "\n",
    "    initial_learning_rate = 0.0002\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 0.96\n",
    "\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate,\n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rate,\n",
    "        staircase=True\n",
    "    )\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=lr_schedule,\n",
    "        beta_1=0.5,\n",
    "        clipnorm=1.0\n",
    "    )\n",
    "\n",
    "    def total_loss(y_true, y_pred):\n",
    "        return combined_generator_loss(y_true, y_pred, d_model(y_pred))\n",
    "\n",
    "    model.compile(\n",
    "        loss=[total_loss, 'binary_crossentropy'],\n",
    "        loss_weights=[100, 1],  # Change the parameter according to the image it's generating\n",
    "        optimizer=optimizer\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def summarize_performance(epoch, g_model, d_model, dataset_colored, dataset_bw, n_samples=100):\n",
    "    X_real, y_real = generate_samples(dataset_colored, n_samples, is_real=True)\n",
    "    _, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
    "\n",
    "    X_fake, y_fake = generate_generator_samples(g_model, dataset_bw, n_samples, is_real=False)\n",
    "    _, acc_fake = d_model.evaluate(X_fake, y_fake, verbose=0)\n",
    "\n",
    "    print(f\"Accuracy real: {acc_real*100:.2f}%, Accuracy fake: {acc_fake*100:.2f}%\")\n",
    "    view_image(X_fake[:5], X_fake[5:10])\n",
    "\n",
    "def train_gan(g_model, d_model, gan_model, dataset_colored, dataset_bw, epochs=50, batch_size=128, checkpoint_dir='./training_checkpoints'):\n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    # Create optimizers for generator and discriminator\n",
    "    g_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    d_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "\n",
    "    # Create checkpoint\n",
    "    checkpoint = tf.train.Checkpoint(\n",
    "        generator_optimizer=g_optimizer,\n",
    "        discriminator_optimizer=d_optimizer,\n",
    "        generator=g_model,\n",
    "        discriminator=d_model\n",
    "    )\n",
    "\n",
    "    # Checkpoint manager to keep multiple checkpoints\n",
    "    checkpoint_manager = tf.train.CheckpointManager(\n",
    "        checkpoint,\n",
    "        directory=checkpoint_dir,\n",
    "        max_to_keep=5  # Keep last 5 checkpoints\n",
    "    )\n",
    "\n",
    "    # Restore the latest checkpoint if exists\n",
    "    latest_checkpoint = checkpoint_manager.latest_checkpoint\n",
    "    if latest_checkpoint:\n",
    "        checkpoint.restore(latest_checkpoint).expect_partial()\n",
    "        print(f\"Resuming training from {latest_checkpoint}\")\n",
    "    else:\n",
    "        print(\"Initializing from scratch.\")\n",
    "\n",
    "    batch_per_epoch = dataset_colored.shape[0] // batch_size\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(batch_per_epoch):\n",
    "            idx = np.random.randint(0, dataset_colored.shape[0], batch_size)\n",
    "            real_images = dataset_colored[idx]\n",
    "            bw_images = dataset_bw[idx]\n",
    "\n",
    "            # Generate fake samples\n",
    "            generated_images = g_model.predict(bw_images, verbose=0)\n",
    "\n",
    "            # Train discriminator\n",
    "            d_loss_real = d_model.train_on_batch(real_images, np.ones((batch_size, 1)))\n",
    "            d_loss_fake = d_model.train_on_batch(generated_images, np.zeros((batch_size, 1)))\n",
    "\n",
    "            # Ensure values are scalars\n",
    "            d_loss_real = d_loss_real[0] if isinstance(d_loss_real, (list, np.ndarray)) else d_loss_real\n",
    "            d_loss_fake = d_loss_fake[0] if isinstance(d_loss_fake, (list, np.ndarray)) else d_loss_fake\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # Train generator\n",
    "            g_loss = gan_model.train_on_batch(bw_images, [real_images, np.ones((batch_size, 1))])\n",
    "            g_total, g_custom, g_bce = g_loss[:3] if isinstance(g_loss, (list, np.ndarray)) else (g_loss, 0, 0)\n",
    "\n",
    "        # Print progress every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch: {epoch+1}/{epochs}\")\n",
    "            print(f\"D1 Loss: {d_loss_real:.4f}, D2 Loss: {d_loss_fake:.4f}\")\n",
    "            print(f\"G Total Loss: {g_total:.4f}, G Custom Loss: {g_custom:.4f}, G BCE Loss: {g_bce:.4f}\")\n",
    "\n",
    "        # Save checkpoint every 50 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            # Save checkpoint\n",
    "            save_path = checkpoint_manager.save()\n",
    "            print(f\"Checkpoint saved: {save_path}\")\n",
    "\n",
    "            idx = np.random.randint(0, dataset_bw.shape[0], 3)\n",
    "            generated_images = g_model.predict(dataset_bw[idx], verbose=0)\n",
    "            view_image(generated_images, dataset_bw[idx])\n",
    "\n",
    "    # Final checkpoint\n",
    "    save_path = checkpoint_manager.save()\n",
    "    print(f\"Training complete. Final checkpoint saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=100 # set as you need\n",
    "batch_size=128\n",
    "shape=(64, 64, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "g_model = generator(shape)\n",
    "d_model = discriminator(shape)\n",
    "gan_model = define_gan(g_model, d_model, shape)\n",
    "\n",
    "# start_time = time.time() # To check how much time its taking\n",
    "train_gan(g_model, d_model, gan_model, train_colored, train_bw, epochs=epochs, batch_size=batch_size)\n",
    "# end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the time\n",
    "# elapsed_time = end_time - start_time\n",
    "# print(f\"Total training time: {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_dir):\n",
    "    # Recreate the generator model with the same architecture\n",
    "    g_model = generator(input_shape=(64, 64, 3))\n",
    "    # Create a checkpoint\n",
    "    checkpoint = tf.train.Checkpoint(generator=g_model)\n",
    "    # Restore the latest checkpoint\n",
    "    checkpoint_manager = tf.train.CheckpointManager(\n",
    "        checkpoint,\n",
    "        directory=checkpoint_dir,\n",
    "        max_to_keep=5\n",
    "    )\n",
    "    latest_checkpoint = checkpoint_manager.latest_checkpoint\n",
    "    if latest_checkpoint:\n",
    "        checkpoint.restore(latest_checkpoint)\n",
    "        print(f\"Restored generator from checkpoint: {latest_checkpoint}\")\n",
    "    else:\n",
    "        print(\"No checkpoint found!\")\n",
    "    return g_model\n",
    "def colorize_images(generator, black_and_white_images):\n",
    "    # Ensure input is in the right shape and normalized\n",
    "    if black_and_white_images.ndim == 3:\n",
    "        black_and_white_images = black_and_white_images[np.newaxis, ...]\n",
    "\n",
    "    # Predict colorized images\n",
    "    colorized_images = generator.predict(black_and_white_images)\n",
    "\n",
    "    return colorized_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "generator_infer = load_checkpoint(checkpoint_dir)\n",
    "test_bw_images = train_bw[:10]\n",
    "colorized_images = colorize_images(generator_infer, test_bw_images)\n",
    "view_image(colorized_images, test_bw_images, num_images=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving and loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(g_model, './model') \n",
    "\n",
    "loaded_model = tf.saved_model.load('./model')\n",
    "infer = loaded_model.signatures[\"serving_default\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(infer.structured_input_signature)\n",
    "\n",
    "input_tensor = tf.convert_to_tensor(train_bw[10:20], dtype=tf.float32)\n",
    "output = infer(inputs=input_tensor)\n",
    "# print(output.keys())\n",
    "generated_image = output[next(iter(output.keys()))].numpy()\n",
    "view_image(generated_image, train_bw[10:20], num_images=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
